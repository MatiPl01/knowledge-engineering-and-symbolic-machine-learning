{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mateu/Education/sem8/ml/project/venv/lib/python3.11/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=0\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: Train Loss = 0.6757, Train Acc = 57.39%  |  Test Loss = 0.6252, Test Acc = 58.81%, Test F1 = 0.348\n",
      "Epoch 02: Train Loss = 0.6388, Train Acc = 64.24%  |  Test Loss = 0.5743, Test Acc = 72.95%, Test F1 = 0.459\n",
      "Epoch 03: Train Loss = 0.5822, Train Acc = 72.44%  |  Test Loss = 0.4560, Test Acc = 85.91%, Test F1 = 0.596\n",
      "Epoch 04: Train Loss = 0.5418, Train Acc = 75.92%  |  Test Loss = 0.4237, Test Acc = 86.33%, Test F1 = 0.598\n",
      "Epoch 05: Train Loss = 0.5263, Train Acc = 76.86%  |  Test Loss = 0.4062, Test Acc = 87.39%, Test F1 = 0.597\n",
      "Epoch 06: Train Loss = 0.5099, Train Acc = 78.31%  |  Test Loss = 0.4272, Test Acc = 86.51%, Test F1 = 0.614\n",
      "Epoch 07: Train Loss = 0.5058, Train Acc = 78.45%  |  Test Loss = 0.4651, Test Acc = 81.64%, Test F1 = 0.563\n",
      "Epoch 08: Train Loss = 0.4966, Train Acc = 79.48%  |  Test Loss = 0.3982, Test Acc = 89.00%, Test F1 = 0.656\n",
      "Epoch 09: Train Loss = 0.4983, Train Acc = 78.93%  |  Test Loss = 0.4252, Test Acc = 88.71%, Test F1 = 0.651\n",
      "Epoch 10: Train Loss = 0.4913, Train Acc = 79.83%  |  Test Loss = 0.3632, Test Acc = 91.70%, Test F1 = 0.701\n",
      "Epoch 11: Train Loss = 0.4768, Train Acc = 81.11%  |  Test Loss = 0.4094, Test Acc = 87.96%, Test F1 = 0.641\n",
      "Epoch 12: Train Loss = 0.4735, Train Acc = 80.87%  |  Test Loss = 0.3867, Test Acc = 89.85%, Test F1 = 0.676\n",
      "Epoch 13: Train Loss = 0.4703, Train Acc = 81.51%  |  Test Loss = 0.3861, Test Acc = 90.27%, Test F1 = 0.686\n",
      "Epoch 14: Train Loss = 0.4652, Train Acc = 81.85%  |  Test Loss = 0.4153, Test Acc = 87.63%, Test F1 = 0.644\n",
      "Epoch 15: Train Loss = 0.4591, Train Acc = 82.52%  |  Test Loss = 0.4056, Test Acc = 87.24%, Test F1 = 0.638\n",
      "Epoch 16: Train Loss = 0.4594, Train Acc = 81.83%  |  Test Loss = 0.3856, Test Acc = 89.97%, Test F1 = 0.684\n",
      "Epoch 17: Train Loss = 0.4572, Train Acc = 82.12%  |  Test Loss = 0.4019, Test Acc = 88.37%, Test F1 = 0.658\n",
      "Epoch 18: Train Loss = 0.4518, Train Acc = 82.54%  |  Test Loss = 0.3630, Test Acc = 90.36%, Test F1 = 0.683\n",
      "Epoch 19: Train Loss = 0.4466, Train Acc = 83.02%  |  Test Loss = 0.3858, Test Acc = 89.74%, Test F1 = 0.682\n",
      "Epoch 20: Train Loss = 0.4377, Train Acc = 84.10%  |  Test Loss = 0.3882, Test Acc = 88.82%, Test F1 = 0.667\n",
      "Epoch 21: Train Loss = 0.4330, Train Acc = 84.14%  |  Test Loss = 0.3870, Test Acc = 88.69%, Test F1 = 0.665\n",
      "Epoch 22: Train Loss = 0.4290, Train Acc = 84.34%  |  Test Loss = 0.3811, Test Acc = 89.13%, Test F1 = 0.671\n",
      "Epoch 23: Train Loss = 0.4301, Train Acc = 84.38%  |  Test Loss = 0.3835, Test Acc = 89.27%, Test F1 = 0.676\n",
      "Epoch 24: Train Loss = 0.4290, Train Acc = 84.63%  |  Test Loss = 0.3772, Test Acc = 89.76%, Test F1 = 0.684\n",
      "Epoch 25: Train Loss = 0.4250, Train Acc = 84.83%  |  Test Loss = 0.3816, Test Acc = 89.29%, Test F1 = 0.676\n",
      "Epoch 26: Train Loss = 0.4268, Train Acc = 84.68%  |  Test Loss = 0.3829, Test Acc = 89.32%, Test F1 = 0.679\n",
      "Epoch 27: Train Loss = 0.4245, Train Acc = 84.81%  |  Test Loss = 0.3901, Test Acc = 88.89%, Test F1 = 0.674\n",
      "Epoch 28: Train Loss = 0.4200, Train Acc = 85.45%  |  Test Loss = 0.3760, Test Acc = 89.70%, Test F1 = 0.683\n",
      "Epoch 29: Train Loss = 0.4249, Train Acc = 84.81%  |  Test Loss = 0.3761, Test Acc = 89.66%, Test F1 = 0.682\n",
      "Epoch 30: Train Loss = 0.4188, Train Acc = 85.37%  |  Test Loss = 0.3923, Test Acc = 88.43%, Test F1 = 0.666\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Set device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Configuration and hyperparameters\n",
    "SR = 16000            # sample rate for audio (convert all audio to 16 kHz)\n",
    "N_MELS = 128          # number of Mel filterbank channels\n",
    "TIME_MASK_PARAM = 30  # max width of time mask (in Mel frames) for SpecAugment\n",
    "FREQ_MASK_PARAM = 8   # max width of frequency mask (in Mel bins) for SpecAugment\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 30\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "DROPOUT_PROB_CONV = 0.2   # dropout after conv layers\n",
    "DROPOUT_PROB_FC = 0.5     # dropout before fully connected layer\n",
    "LSTM_HIDDEN = 128         # hidden size of LSTM\n",
    "LSTM_LAYERS = 2           # number of LSTM layers\n",
    "BIDIRECTIONAL = True      # use bi-directional LSTM\n",
    "\n",
    "# Dataset definition\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, data_dir, subset=\"train\"):\n",
    "        \"\"\"\n",
    "        Dataset for audio question detection.\n",
    "        data_dir: path to dataset (contains 'questions' and 'others' subfolders)\n",
    "        subset: \"train\" or \"test\", used for toggling augmentation.\n",
    "        \"\"\"\n",
    "        self.data_dir = data_dir\n",
    "        self.subset = subset\n",
    "        # Map subfolders to labels\n",
    "        self.label_map = {\"others\": 0, \"questions\": 1}\n",
    "        self.file_list = []  # list of (filepath, label)\n",
    "        for class_name, label in self.label_map.items():\n",
    "            class_dir = os.path.join(data_dir, class_name)\n",
    "            if not os.path.isdir(class_dir):\n",
    "                continue\n",
    "            for fname in os.listdir(class_dir):\n",
    "                if fname.lower().endswith(\".wav\"):\n",
    "                    self.file_list.append((os.path.join(class_dir, fname), label))\n",
    "        # If training set, compute global mean and std for normalization\n",
    "        if subset == \"train\":\n",
    "            self.global_mean, self.global_std = self._compute_global_norm_stats()\n",
    "        else:\n",
    "            # For test, we expect to use the same normalization as computed on train\n",
    "            # So we require train stats to have been computed externally and saved or passed in.\n",
    "            # For simplicity here, we'll assume train dataset was initialized first and set class variables.\n",
    "            # (In practice, you might load precomputed mean/std values.)\n",
    "            if hasattr(AudioDataset, \"train_mean\") and hasattr(AudioDataset, \"train_std\"):\n",
    "                self.global_mean = AudioDataset.train_mean\n",
    "                self.global_std = AudioDataset.train_std\n",
    "            else:\n",
    "                raise RuntimeError(\"Training statistics not found for normalization.\")\n",
    "\n",
    "    def _compute_global_norm_stats(self):\n",
    "        \"\"\"Compute global mean and std over all spectrogram pixels in the training set.\"\"\"\n",
    "        sum_val = 0.0\n",
    "        sum_sq_val = 0.0\n",
    "        count = 0\n",
    "        for filepath, _ in self.file_list:\n",
    "            # Load audio\n",
    "            y, sr = librosa.load(filepath, sr=SR)\n",
    "            # Compute Mel spectrogram (power) and convert to dB\n",
    "            mel_spec = librosa.feature.melspectrogram(\n",
    "                y=y, sr=SR, n_mels=N_MELS, power=2.0\n",
    "            )\n",
    "            mel_db = librosa.power_to_db(mel_spec, top_db=80)\n",
    "            # Flatten to 1D and accumulate statistics\n",
    "            mel_db_flat = mel_db.flatten()\n",
    "            sum_val += mel_db_flat.sum()\n",
    "            sum_sq_val += np.square(mel_db_flat).sum()\n",
    "            count += mel_db_flat.size\n",
    "        global_mean = sum_val / count\n",
    "        global_var = sum_sq_val / count - (global_mean ** 2)\n",
    "        global_std = np.sqrt(global_var)\n",
    "        # Save for reuse in test dataset\n",
    "        AudioDataset.train_mean = global_mean\n",
    "        AudioDataset.train_std = global_std\n",
    "        return global_mean, global_std\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filepath, label = self.file_list[idx]\n",
    "        # Load audio file and compute Mel spectrogram\n",
    "        y, sr = librosa.load(filepath, sr=SR)\n",
    "        mel_spec = librosa.feature.melspectrogram(y=y, sr=SR, n_mels=N_MELS, power=2.0)\n",
    "        mel_db = librosa.power_to_db(mel_spec, top_db=80)\n",
    "        # Normalize spectrogram using global mean and std (computed from train set)\n",
    "        mel_db_norm = (mel_db - self.global_mean) / (self.global_std + 1e-6)\n",
    "        # Convert to torch tensor\n",
    "        mel_tensor = torch.tensor(mel_db_norm, dtype=torch.float32)\n",
    "        # For consistency, add channel dimension (1 channel for Mel spectrogram)\n",
    "        mel_tensor = mel_tensor.unsqueeze(0)  # shape: (1, N_MELS, time_frames)\n",
    "        # Apply SpecAugment (time and frequency masking) only on training data\n",
    "        if self.subset == \"train\":\n",
    "            # Define masking transforms (these use random masks each call)\n",
    "            time_mask = torchaudio.transforms.TimeMasking(time_mask_param=TIME_MASK_PARAM)\n",
    "            freq_mask = torchaudio.transforms.FrequencyMasking(freq_mask_param=FREQ_MASK_PARAM)\n",
    "            mel_tensor = time_mask(mel_tensor)\n",
    "            mel_tensor = freq_mask(mel_tensor)\n",
    "        return mel_tensor, label\n",
    "\n",
    "# Collate function for DataLoader to pad variable-length sequences\n",
    "def pad_collate(batch):\n",
    "    \"\"\"\n",
    "    Pad spectrograms in the batch to the same time dimension.\n",
    "    This will pad with zeros (which, after normalization, correspond to silence).\n",
    "    \"\"\"\n",
    "    # Batch is a list of (spec_tensor, label)\n",
    "    # Find max time length in this batch\n",
    "    max_frames = max(item[0].shape[-1] for item in batch)\n",
    "    specs = []\n",
    "    labels = []\n",
    "    for spec, label in batch:\n",
    "        seq_len = spec.shape[-1]\n",
    "        if seq_len < max_frames:\n",
    "            # Pad with zeros on the time axis\n",
    "            pad_width = max_frames - seq_len\n",
    "            # pad shape: (1, N_MELS, pad_width)\n",
    "            pad_tensor = torch.zeros((spec.shape[0], spec.shape[1], pad_width), dtype=torch.float32)\n",
    "            spec_padded = torch.cat([spec, pad_tensor], dim=-1)\n",
    "        else:\n",
    "            spec_padded = spec\n",
    "        specs.append(spec_padded)\n",
    "        labels.append(label)\n",
    "    # Stack into batch tensors\n",
    "    specs = torch.stack(specs)      # shape: (batch, 1, N_MELS, max_frames)\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    return specs, labels\n",
    "\n",
    "\n",
    "class CRNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, 1, 1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Conv2d(32, 64, 3, 1, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "        # build a dummy to find (channels * freq_out)\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, 1, N_MELS, 100)  # 100 time frames is arbitrary\n",
    "            c_out, f_out = self.conv(dummy).shape[1:3]  # (batch, C, F, T) → C,F\n",
    "        feat_dim = c_out * f_out\n",
    "        self.lstm = nn.LSTM(\n",
    "            feat_dim,\n",
    "            LSTM_HIDDEN,\n",
    "            num_layers=LSTM_LAYERS,\n",
    "            batch_first=True,\n",
    "            bidirectional=BIDIRECTIONAL,\n",
    "            dropout=0.3,\n",
    "        )\n",
    "        self.dropout_fc = nn.Dropout(DROPOUT_PROB_FC)\n",
    "        self.fc = nn.Linear(LSTM_HIDDEN * (2 if BIDIRECTIONAL else 1), 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)  # (B,C,F,T)\n",
    "        B, C, F, T = x.shape\n",
    "        x = x.permute(0, 3, 1, 2).reshape(B, T, C * F)\n",
    "        h, _ = self.lstm(x)\n",
    "        if BIDIRECTIONAL:\n",
    "            h_last = torch.cat([h[:, -1, :LSTM_HIDDEN], h[:, 0, LSTM_HIDDEN:]], dim=1)\n",
    "        else:\n",
    "            h_last = h[:, -1]\n",
    "        return self.fc(self.dropout_fc(h_last))\n",
    "\n",
    "# Initialize datasets and data loaders\n",
    "train_dataset = AudioDataset(data_dir=\"cleaned_dataset/train\", subset=\"train\")\n",
    "test_dataset  = AudioDataset(data_dir=\"cleaned_dataset/test\",  subset=\"test\")\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=pad_collate)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=pad_collate)\n",
    "\n",
    "# Initialize model, loss, optimizer, scheduler\n",
    "model = CRNNModel().to(device)\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)  # using label smoothing for stability\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)  # decay LR after 10 epochs\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    for specs, labels in train_loader:\n",
    "        specs = specs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(specs)                # forward pass\n",
    "        loss = criterion(outputs, labels)     # compute loss (with label smoothing)\n",
    "        loss.backward()                       # backpropagate\n",
    "        optimizer.step()\n",
    "        # accumulate training stats\n",
    "        running_loss += loss.item() * specs.size(0)\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "        total_train += labels.size(0)\n",
    "    train_loss = running_loss / total_train\n",
    "    train_acc = correct_train / total_train\n",
    "\n",
    "    # Evaluation on test set\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "    # For F1 score calculation:\n",
    "    true_positives = true_negatives = false_positives = false_negatives = 0\n",
    "    with torch.no_grad():\n",
    "        for specs, labels in test_loader:\n",
    "            specs = specs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(specs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item() * specs.size(0)\n",
    "            # Predicted class\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            correct_test += (predicted == labels).sum().item()\n",
    "            total_test += labels.size(0)\n",
    "            # F1 components\n",
    "            for i in range(labels.size(0)):\n",
    "                if predicted[i] == 1 and labels[i] == 1:\n",
    "                    true_positives += 1\n",
    "                elif predicted[i] == 1 and labels[i] == 0:\n",
    "                    false_positives += 1\n",
    "                elif predicted[i] == 0 and labels[i] == 1:\n",
    "                    false_negatives += 1\n",
    "                # (predicted==0 and label==0 -> true_negatives, not needed for F1)\n",
    "    test_loss = test_loss / total_test\n",
    "    test_acc = correct_test / total_test\n",
    "    # Compute F1 (binary classification: treat \"question\" as positive class)\n",
    "    if true_positives == 0:\n",
    "        # Avoid division by zero: if no positive predictions or true positives\n",
    "        precision = 0.0\n",
    "        recall = 0.0\n",
    "        f1 = 0.0\n",
    "    else:\n",
    "        precision = true_positives / (true_positives + false_positives + 1e-8)\n",
    "        recall = true_positives / (true_positives + false_negatives + 1e-8)\n",
    "        if precision + recall == 0:\n",
    "            f1 = 0.0\n",
    "        else:\n",
    "            f1 = 2 * precision * recall / (precision + recall)\n",
    "    # Step the learning rate scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "    # Print epoch summary\n",
    "    print(f\"Epoch {epoch:02d}: \"\n",
    "          f\"Train Loss = {train_loss:.4f}, Train Acc = {train_acc*100:.2f}%  |  \"\n",
    "          f\"Test Loss = {test_loss:.4f}, Test Acc = {test_acc*100:.2f}%, Test F1 = {f1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
