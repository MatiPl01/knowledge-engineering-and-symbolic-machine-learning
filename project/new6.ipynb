{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mateu/Education/sem8/ml/project/venv/lib/python3.11/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=0\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global mean: -62.0506, Global std: 19.5367\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import librosa\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Dataset class for audio question detection\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, root_dir, split, mean=None, std=None):\n",
    "        self.files = []\n",
    "        self.labels = []\n",
    "        # Traverse the 'questions' and 'others' subdirectories\n",
    "        for label_name in [\"questions\", \"others\"]:\n",
    "            class_dir = os.path.join(root_dir, split, label_name)\n",
    "            if not os.path.isdir(class_dir):\n",
    "                continue\n",
    "            for fname in os.listdir(class_dir):\n",
    "                if fname.endswith(\".wav\"):\n",
    "                    self.files.append(os.path.join(class_dir, fname))\n",
    "                    # Label: 1 for question, 0 for other\n",
    "                    self.labels.append(1 if label_name == \"questions\" else 0)\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.files[idx]\n",
    "        # Load audio (use original sampling rate or specify one, e.g., sr=16000)\n",
    "        waveform, sr = librosa.load(file_path, sr=None)\n",
    "        # Compute power Mel spectrogram (128 Mel bins)\n",
    "        S = librosa.feature.melspectrogram(y=waveform, sr=sr, n_mels=128, power=2.0)\n",
    "        # Convert to log-scale (dB). ref=np.max sets 0 dB to the peak power in this spectrogram.\n",
    "        S_db = librosa.power_to_db(S, top_db=80.0, ref=np.max)\n",
    "        # Normalize using global mean and std if provided\n",
    "        if self.mean is not None and self.std is not None:\n",
    "            S_db = (S_db - self.mean) / (self.std + 1e-8)\n",
    "        # Convert to torch tensor (time_frames, 128)\n",
    "        # Note: librosa returns shape (128, time_frames), so transpose to (time_frames, 128)\n",
    "        spectrogram = torch.tensor(S_db.T, dtype=torch.float32)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return spectrogram, label\n",
    "\n",
    "# Collate function to pad variable-length spectrograms in a batch\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Pads a list of (spectrogram, label) tuples to the same length.\n",
    "    Returns:\n",
    "      - batch_spectrogram: Tensor of shape (B, T_max, 128)\n",
    "      - batch_labels: Tensor of shape (B,)\n",
    "      - batch_mask: Boolean mask of shape (B, T_max) where False indicates real data and True indicates padding.\n",
    "    \"\"\"\n",
    "    # Get sequence lengths for each sample\n",
    "    lengths = [spec.shape[0] for spec, _ in batch]\n",
    "    max_len = max(lengths)\n",
    "    batch_size = len(batch)\n",
    "    # Initialize padded tensor and mask\n",
    "    batch_spectrogram = torch.zeros((batch_size, max_len, 128), dtype=torch.float32)\n",
    "    # Mask with True for padding positions (will be used to ignore pads in attention)\n",
    "    batch_mask = torch.ones((batch_size, max_len), dtype=torch.bool)\n",
    "    batch_labels = torch.zeros((batch_size,), dtype=torch.long)\n",
    "    for i, (spec, label) in enumerate(batch):\n",
    "        L = spec.shape[0]\n",
    "        batch_spectrogram[i, :L] = spec        # pad with zeros beyond L\n",
    "        batch_mask[i, :L] = False             # False means non-padded (actual data)\n",
    "        batch_labels[i] = label\n",
    "    return batch_spectrogram, batch_labels, batch_mask\n",
    "\n",
    "# Load training set without normalization to compute global mean and std\n",
    "train_dataset_temp = AudioDataset(root_dir=\"cleaned_dataset\", split=\"train\", mean=None, std=None)\n",
    "# Compute global mean and std across all training spectrogram values\n",
    "sum_val, sum_sq_val, count = 0.0, 0.0, 0\n",
    "for spectrogram, _ in train_dataset_temp:\n",
    "    # spectrogram is a tensor shape (T, 128)\n",
    "    sum_val += spectrogram.sum().item()\n",
    "    sum_sq_val += (spectrogram ** 2).sum().item()\n",
    "    count += spectrogram.numel()\n",
    "global_mean = sum_val / count\n",
    "global_var = sum_sq_val / count - (global_mean ** 2)\n",
    "global_std = math.sqrt(global_var)  # standard deviation\n",
    "\n",
    "print(f\"Global mean: {global_mean:.4f}, Global std: {global_std:.4f}\")\n",
    "\n",
    "# Now create Dataset objects with normalization\n",
    "train_dataset = AudioDataset(root_dir=\"cleaned_dataset\", split=\"train\", mean=global_mean, std=global_std)\n",
    "test_dataset  = AudioDataset(root_dir=\"cleaned_dataset\", split=\"test\",  mean=global_mean, std=global_std)\n",
    "\n",
    "# DataLoaders for batching\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# Positional Encoding module (sinusoidal)\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000, dropout=0.1):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        # Create constant positional encoding matrix (1, max_len, d_model)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        # Compute sinusoidal functions of different frequencies for each dimension:\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(\n",
    "            position * div_term\n",
    "        )  # apply sin to even indices in the encoding\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # apply cos to odd indices\n",
    "        pe = pe.unsqueeze(0)  # shape (1, max_len, d_model)\n",
    "        self.register_buffer(\"pe\", pe)  # register as buffer so it's not a parameter\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, d_model)\n",
    "        # Add positional encoding up to the sequence length of x\n",
    "        seq_len = x.size(1)\n",
    "        x = x + self.pe[:, :seq_len].to(x.device)\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "# Transformer-based audio classifier\n",
    "class AudioTransformerClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model=64,\n",
    "        nhead=4,\n",
    "        num_layers=2,\n",
    "        dim_feedforward=128,\n",
    "        dropout=0.1,\n",
    "        pooling=\"mean\",\n",
    "    ):\n",
    "        super(AudioTransformerClassifier, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.pooling = pooling\n",
    "        # Project 128-dim Mel frame to d_model dimensions\n",
    "        self.frame_projection = nn.Linear(128, d_model)\n",
    "        # Positional encoding for sequence ordering\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout=dropout)\n",
    "        # Transformer Encoder stack\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "        )  # batch_first allows input as (batch, seq, embed)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer, num_layers=num_layers\n",
    "        )\n",
    "        # Classification MLP head\n",
    "        self.fc1 = nn.Linear(d_model, d_model // 2)  # hidden layer (reduce dimension)\n",
    "        self.fc2 = nn.Linear(d_model // 2, 2)  # 2 output classes\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, src_key_padding_mask=None):\n",
    "        \"\"\"\n",
    "        x: Tensor of shape (batch, seq_len, 128) - input log-Mel spectrogram sequence.\n",
    "        src_key_padding_mask: Boolean mask of shape (batch, seq_len) with True for padded positions.\n",
    "        \"\"\"\n",
    "        # 1. Linear projection (embed each frame) and scale by sqrt(d_model)\n",
    "        x = self.frame_projection(x) * math.sqrt(self.d_model)\n",
    "        # 2. Add positional encodings\n",
    "        x = self.pos_encoder(x)\n",
    "        # 3. Transformer encoder (will attend to non-padded frames based on the mask)\n",
    "        # src_key_padding_mask: True values are ignored (pad positions)\n",
    "        encoded_seq = self.transformer_encoder(\n",
    "            x, src_key_padding_mask=src_key_padding_mask\n",
    "        )\n",
    "        # 4. Pooling over time frames to get a single vector representation\n",
    "        if self.pooling == \"mean\":\n",
    "            # Mean pooling (exclude padded frames from the mean)\n",
    "            if src_key_padding_mask is not None:\n",
    "                # mask: False for real frames, True for pads. Invert to 1 for real frames:\n",
    "                mask = (\n",
    "                    (~src_key_padding_mask).unsqueeze(-1).float()\n",
    "                )  # shape (batch, seq_len, 1)\n",
    "                # Multiply to zero-out padded frames, then sum and divide by actual lengths\n",
    "                summed = (encoded_seq * mask).sum(\n",
    "                    dim=1\n",
    "                )  # sum over time dim -> shape (batch, d_model)\n",
    "                lengths = mask.sum(\n",
    "                    dim=1\n",
    "                )  # shape (batch, 1), number of real frames per sample\n",
    "                lengths[lengths == 0] = 1.0  # avoid division by zero\n",
    "                pooled = summed / lengths\n",
    "            else:\n",
    "                # If no mask provided, just average across all frames\n",
    "                pooled = encoded_seq.mean(dim=1)\n",
    "        elif self.pooling == \"last\":\n",
    "            # Use the output of the last real frame (for each sequence)\n",
    "            if src_key_padding_mask is not None:\n",
    "                lengths = (~src_key_padding_mask).sum(\n",
    "                    dim=1\n",
    "                )  # number of real (non-pad) frames for each sample\n",
    "            else:\n",
    "                lengths = torch.tensor(\n",
    "                    [encoded_seq.size(1)] * encoded_seq.size(0), device=x.device\n",
    "                )\n",
    "            # Clamp minimum length to 1 to avoid invalid indexing if any sequence is empty\n",
    "            lengths = torch.clamp(lengths, min=1)\n",
    "            # Gather the last output for each sample\n",
    "            # Construct index tensor of shape (batch, 1, d_model) for gathering the last frame\n",
    "            idx = (lengths - 1).view(-1, 1, 1).expand(-1, 1, encoded_seq.size(2))\n",
    "            pooled = encoded_seq.gather(dim=1, index=idx).squeeze(\n",
    "                1\n",
    "            )  # shape (batch, d_model)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown pooling type: {self.pooling}\")\n",
    "        # 5. Classification MLP: [d_model] -> [d_model/2] -> [2]\n",
    "        x = self.dropout(pooled)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        logits = self.fc2(x)  # raw scores for the two classes\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mateu/Education/sem8/ml/project/venv/lib/python3.11/site-packages/torch/nn/modules/transformer.py:505: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/NestedTensorImpl.cpp:182.)\n",
      "  output = torch._nested_tensor_from_mask(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: TrainLoss=0.6303, TestAcc=0.850, TestF1=0.127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mateu/Education/sem8/ml/project/venv/lib/python3.11/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=0\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 02: TrainLoss=0.6065, TestAcc=0.832, TestF1=0.395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mateu/Education/sem8/ml/project/venv/lib/python3.11/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=0\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 03: TrainLoss=0.6000, TestAcc=0.847, TestF1=0.397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mateu/Education/sem8/ml/project/venv/lib/python3.11/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=0\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 04: TrainLoss=0.5762, TestAcc=0.637, TestF1=0.419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mateu/Education/sem8/ml/project/venv/lib/python3.11/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=0\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 05: TrainLoss=0.5405, TestAcc=0.824, TestF1=0.489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mateu/Education/sem8/ml/project/venv/lib/python3.11/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=0\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 06: TrainLoss=0.5275, TestAcc=0.865, TestF1=0.289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mateu/Education/sem8/ml/project/venv/lib/python3.11/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=0\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 07: TrainLoss=0.5159, TestAcc=0.869, TestF1=0.502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mateu/Education/sem8/ml/project/venv/lib/python3.11/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=0\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 08: TrainLoss=0.5054, TestAcc=0.831, TestF1=0.515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mateu/Education/sem8/ml/project/venv/lib/python3.11/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=0\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 09: TrainLoss=0.5002, TestAcc=0.850, TestF1=0.497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mateu/Education/sem8/ml/project/venv/lib/python3.11/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=0\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: TrainLoss=0.5106, TestAcc=0.529, TestF1=0.371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mateu/Education/sem8/ml/project/venv/lib/python3.11/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=0\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: TrainLoss=0.4845, TestAcc=0.761, TestF1=0.490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mateu/Education/sem8/ml/project/venv/lib/python3.11/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=0\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: TrainLoss=0.4680, TestAcc=0.762, TestF1=0.492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mateu/Education/sem8/ml/project/venv/lib/python3.11/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=0\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: TrainLoss=0.4504, TestAcc=0.773, TestF1=0.497\n",
      "Early stopping triggered - no improvement in F1 for 5 epochs.\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "# Initialize model, optimizer, loss, and scheduler\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AudioTransformerClassifier(\n",
    "    d_model=64, nhead=4, num_layers=2, dim_feedforward=128, dropout=0.1, pooling=\"mean\"\n",
    ").to(device)\n",
    "criterion = nn.CrossEntropyLoss()  # for binary classification (with logits of size 2)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "# Reduce LR by factor of 0.5 if F1 doesn't improve for 2 epochs\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"max\", factor=0.5, patience=2\n",
    ")\n",
    "\n",
    "num_epochs = 20\n",
    "patience = 5  # early stopping patience\n",
    "\n",
    "best_f1 = 0.0\n",
    "epochs_no_improve = 0\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    # Training loop\n",
    "    for batch_x, batch_y, batch_mask in train_loader:\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "        batch_mask = batch_mask.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        logits = model(batch_x, src_key_padding_mask=batch_mask)\n",
    "        loss = criterion(logits, batch_y)\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * batch_x.size(0)\n",
    "    avg_train_loss = running_loss / len(train_loader.dataset)\n",
    "    # Evaluation on test set\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y, batch_mask in test_loader:\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            batch_mask = batch_mask.to(device)\n",
    "            logits = model(batch_x, src_key_padding_mask=batch_mask)\n",
    "            # Predicted class is the index of max logit\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "            all_labels.append(batch_y.cpu().numpy())\n",
    "    # Concatenate all batches\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    # Compute accuracy and F1\n",
    "    epoch_acc = accuracy_score(all_labels, all_preds)\n",
    "    # Compute F1-score for class \"question\" (label 1)\n",
    "    epoch_f1 = f1_score(all_labels, all_preds, average=\"binary\", pos_label=1)\n",
    "    # Print epoch summary\n",
    "    print(\n",
    "        f\"Epoch {epoch:02d}: TrainLoss={avg_train_loss:.4f}, TestAcc={epoch_acc:.3f}, TestF1={epoch_f1:.3f}\"\n",
    "    )\n",
    "    # Scheduler step on F1 (to maximize, use mode='max')\n",
    "    scheduler.step(epoch_f1)\n",
    "    # Early stopping check\n",
    "    if epoch_f1 > best_f1:\n",
    "        best_f1 = epoch_f1\n",
    "        epochs_no_improve = 0\n",
    "        # Save the best model weights (optional)\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(\n",
    "                \"Early stopping triggered - no improvement in F1 for {} epochs.\".format(\n",
    "                    patience\n",
    "                )\n",
    "            )\n",
    "            break\n",
    "\n",
    "# Load best model for further use (if saved)\n",
    "# model.load_state_dict(torch.load(\"best_model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mateu/Education/sem8/ml/project/venv/lib/python3.11/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=0\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed global_mean=-62.0506, global_std=19.5367\n",
      "Epoch 01: TrainLoss=0.6336, TestAcc=0.779, TestF1=0.395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mateu/Education/sem8/ml/project/venv/lib/python3.11/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=0\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 02: TrainLoss=0.6026, TestAcc=0.784, TestF1=0.426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mateu/Education/sem8/ml/project/venv/lib/python3.11/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=0\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 03: TrainLoss=0.5785, TestAcc=0.742, TestF1=0.462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mateu/Education/sem8/ml/project/venv/lib/python3.11/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=0\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 04: TrainLoss=0.5459, TestAcc=0.678, TestF1=0.434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mateu/Education/sem8/ml/project/venv/lib/python3.11/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=0\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 05: TrainLoss=0.5318, TestAcc=0.780, TestF1=0.490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mateu/Education/sem8/ml/project/venv/lib/python3.11/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=0\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 06: TrainLoss=0.5103, TestAcc=0.707, TestF1=0.439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mateu/Education/sem8/ml/project/venv/lib/python3.11/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=0\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 07: TrainLoss=0.5176, TestAcc=0.662, TestF1=0.437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mateu/Education/sem8/ml/project/venv/lib/python3.11/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=0\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 08: TrainLoss=0.5044, TestAcc=0.787, TestF1=0.508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mateu/Education/sem8/ml/project/venv/lib/python3.11/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=0\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 09: TrainLoss=0.4854, TestAcc=0.848, TestF1=0.529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mateu/Education/sem8/ml/project/venv/lib/python3.11/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=0\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: TrainLoss=0.4700, TestAcc=0.783, TestF1=0.496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mateu/Education/sem8/ml/project/venv/lib/python3.11/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=0\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: TrainLoss=0.4652, TestAcc=0.835, TestF1=0.532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mateu/Education/sem8/ml/project/venv/lib/python3.11/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=0\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: TrainLoss=0.4661, TestAcc=0.799, TestF1=0.509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mateu/Education/sem8/ml/project/venv/lib/python3.11/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=0\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: TrainLoss=0.4554, TestAcc=0.871, TestF1=0.512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mateu/Education/sem8/ml/project/venv/lib/python3.11/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=0\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: TrainLoss=0.4562, TestAcc=0.506, TestF1=0.368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mateu/Education/sem8/ml/project/venv/lib/python3.11/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=0\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: TrainLoss=0.4402, TestAcc=0.778, TestF1=0.509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mateu/Education/sem8/ml/project/venv/lib/python3.11/site-packages/librosa/core/spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=0\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: TrainLoss=0.4136, TestAcc=0.775, TestF1=0.512\n",
      "Early stopping: no improvement after 5 epochs.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import librosa\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "\n",
    "# 1. Dataset and DataLoader setup\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, root_dir, split, mean=None, std=None):\n",
    "        self.file_paths = []\n",
    "        self.labels = []\n",
    "        for label_name in [\"questions\", \"others\"]:\n",
    "            class_dir = os.path.join(root_dir, split, label_name)\n",
    "            if not os.path.isdir(class_dir):\n",
    "                continue\n",
    "            for fname in os.listdir(class_dir):\n",
    "                if fname.endswith(\".wav\"):\n",
    "                    self.file_paths.append(os.path.join(class_dir, fname))\n",
    "                    self.labels.append(1 if label_name == \"questions\" else 0)\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.file_paths[idx]\n",
    "        waveform, sr = librosa.load(path, sr=None)\n",
    "        S = librosa.feature.melspectrogram(y=waveform, sr=sr, n_mels=128, power=2.0)\n",
    "        S_db = librosa.power_to_db(S, top_db=80.0, ref=np.max)\n",
    "        if self.mean is not None and self.std is not None:\n",
    "            S_db = (S_db - self.mean) / (self.std + 1e-8)\n",
    "        spectrogram = torch.tensor(S_db.T, dtype=torch.float32)  # shape (T, 128)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return spectrogram, label\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    lengths = [spec.shape[0] for spec, _ in batch]\n",
    "    max_len = max(lengths)\n",
    "    batch_size = len(batch)\n",
    "    batch_specs = torch.zeros((batch_size, max_len, 128), dtype=torch.float32)\n",
    "    batch_mask = torch.ones((batch_size, max_len), dtype=torch.bool)  # True = PAD\n",
    "    batch_labels = torch.zeros((batch_size,), dtype=torch.long)\n",
    "    for i, (spec, label) in enumerate(batch):\n",
    "        L = spec.shape[0]\n",
    "        batch_specs[i, :L] = spec\n",
    "        batch_mask[i, :L] = False  # False = actual data\n",
    "        batch_labels[i] = label\n",
    "    return batch_specs, batch_labels, batch_mask\n",
    "\n",
    "\n",
    "# Compute global mean and std from training data\n",
    "train_temp = AudioDataset(\"cleaned_dataset\", \"train\", mean=None, std=None)\n",
    "sum_val, sum_sq_val, count = 0.0, 0.0, 0\n",
    "for spec, _ in train_temp:\n",
    "    sum_val += spec.sum().item()\n",
    "    sum_sq_val += (spec**2).sum().item()\n",
    "    count += spec.numel()\n",
    "global_mean = sum_val / count\n",
    "global_std = math.sqrt(max(sum_sq_val / count - global_mean**2, 0.0))\n",
    "print(f\"Computed global_mean={global_mean:.4f}, global_std={global_std:.4f}\")\n",
    "\n",
    "# Create normalized datasets and loaders\n",
    "train_dataset = AudioDataset(\n",
    "    \"cleaned_dataset\", \"train\", mean=global_mean, std=global_std\n",
    ")\n",
    "test_dataset = AudioDataset(\"cleaned_dataset\", \"test\", mean=global_mean, std=global_std)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "\n",
    "# 2. Model definition\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000, dropout=0.1):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # shape (1, max_len, d_model)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, : x.size(1)].to(x.device)\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class AudioTransformerClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model=64,\n",
    "        nhead=4,\n",
    "        num_layers=2,\n",
    "        dim_feedforward=128,\n",
    "        dropout=0.1,\n",
    "        pooling=\"mean\",\n",
    "    ):\n",
    "        super(AudioTransformerClassifier, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.pooling = pooling\n",
    "        self.frame_projection = nn.Linear(128, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout=dropout)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer, num_layers=num_layers\n",
    "        )\n",
    "        # Classification MLP\n",
    "        self.fc1 = nn.Linear(d_model, d_model // 2)\n",
    "        self.fc2 = nn.Linear(d_model // 2, 2)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, src_key_padding_mask=None):\n",
    "        # x: (batch, seq_len, 128)\n",
    "        x = self.frame_projection(x) * math.sqrt(self.d_model)\n",
    "        x = self.pos_encoder(x)\n",
    "        encoded = self.transformer_encoder(x, src_key_padding_mask=src_key_padding_mask)\n",
    "        if self.pooling == \"mean\":\n",
    "            if src_key_padding_mask is not None:\n",
    "                mask = (~src_key_padding_mask).unsqueeze(-1).float()\n",
    "                summed = (encoded * mask).sum(dim=1)\n",
    "                lengths = mask.sum(dim=1)\n",
    "                lengths[lengths == 0] = 1.0\n",
    "                pooled = summed / lengths\n",
    "            else:\n",
    "                pooled = encoded.mean(dim=1)\n",
    "        elif self.pooling == \"last\":\n",
    "            if src_key_padding_mask is not None:\n",
    "                lengths = (~src_key_padding_mask).sum(dim=1)\n",
    "            else:\n",
    "                lengths = torch.tensor(\n",
    "                    [encoded.size(1)] * encoded.size(0), device=x.device\n",
    "                )\n",
    "            lengths = torch.clamp(lengths, min=1)\n",
    "            idx = (lengths - 1).view(-1, 1, 1).expand(-1, 1, encoded.size(2))\n",
    "            pooled = encoded.gather(dim=1, index=idx).squeeze(1)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown pooling type\")\n",
    "        x = self.dropout(pooled)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        logits = self.fc2(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# 3. Training setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AudioTransformerClassifier(\n",
    "    d_model=64, nhead=4, num_layers=2, dim_feedforward=128, dropout=0.1, pooling=\"mean\"\n",
    ").to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"max\", factor=0.5, patience=2\n",
    ")\n",
    "\n",
    "num_epochs = 20\n",
    "patience = 5\n",
    "best_f1 = 0.0\n",
    "epochs_no_improve = 0\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch_x, batch_y, batch_mask in train_loader:\n",
    "        batch_x, batch_y, batch_mask = (\n",
    "            batch_x.to(device),\n",
    "            batch_y.to(device),\n",
    "            batch_mask.to(device),\n",
    "        )\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_x, src_key_padding_mask=batch_mask)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * batch_x.size(0)\n",
    "    avg_loss = total_loss / len(train_loader.dataset)\n",
    "    # Evaluate on test set\n",
    "    model.eval()\n",
    "    all_preds, all_targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y, batch_mask in test_loader:\n",
    "            batch_x, batch_y, batch_mask = (\n",
    "                batch_x.to(device),\n",
    "                batch_y.to(device),\n",
    "                batch_mask.to(device),\n",
    "            )\n",
    "            outputs = model(batch_x, src_key_padding_mask=batch_mask)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_targets.extend(batch_y.cpu().numpy())\n",
    "    acc = accuracy_score(all_targets, all_preds)\n",
    "    f1 = f1_score(all_targets, all_preds, average=\"binary\", pos_label=1)\n",
    "    print(\n",
    "        f\"Epoch {epoch:02d}: TrainLoss={avg_loss:.4f}, TestAcc={acc:.3f}, TestF1={f1:.3f}\"\n",
    "    )\n",
    "    scheduler.step(f1)\n",
    "    # Check for improvement\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        epochs_no_improve = 0\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(\"Early stopping: no improvement after %d epochs.\" % patience)\n",
    "            break\n",
    "\n",
    "# Training complete. The best model is saved in 'best_model.pth'."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
